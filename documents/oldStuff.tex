\section{Preliminaries}
\label{sec:preliminaries}

\subsection{The problem}
In this section, we introduce our notations and formalize Subgroup
Discovery. We introduce two ``flavours'' of the problem. In
the first variant, we seek the \emph{k best} subgroups. In the second one,
we seek \emph{k diverse} sugroups.

\subsubsection{Definitions}
Consider a database $DB$. We use the expressions \emph{variable} and
\emph{column} indifferently. The set $\Gamma$
represents the $|\Gamma|$ non-target variables, and $T$ represents the target
variable.

Our aim is to discover interesting sugroups. A subgroup is a set of tuples $G
\subset DB$, defined by a \emph{description} $d(G)$. In principle, the
description is expressed in a language chosen by the user. In this paper, we
restrict the language to conjunctions (e.g., $\text{age} > 40 \wedge
\text{size} < 180$). We denote $\gamma(G)$ the variables which appear in $G$'s
description (e.g., \{age, size\}).

To evaluate a subgroup, we need a \emph{quality measure}. A quality measure is
a function $\varphi : L \to \mathbb{R}$, where $L$ is the language. This
function assigns a score to each subgroup.  By convention, higher is better.

\subsubsection{Objective}
We can now define our two problems. The first problem describes the ``usual'' Subgroup
Discovery task:

\begin{problem}
    (Top-k subgroup discovery) Given a database $DB$, a quality measure
    $\varphi$ and an integer $k$, find the $k$ best subgroups with regards to
    $\varphi$.
\end{problem}

This approach is based on local criteria: the aim is to maximize the quality
for each subgroup separately. As pointed out by out  \cite{van2011non}, this
approach tends to produce highly redundant patterns sets. To avoid this, we use
a \emph{global} approach: 

\begin{problem}
    (Diverse-k subgroup discovery) Given a database $DB$, a quality measure
    $\varphi$ and an integer $k$, find a non redundant set of $k$ high quality subgroups
\end{problem}

As the authors of the original paper point out, defining redundancy 
is non trivial. In our work, we consider that two subgroup are redundant when
their \emph{description} overlap. [there are other definitions in the paper]

\subsection{Quality measures}
In this section, we describe a few usual quality measures. This includes (but
is not limited to ) the Weighted Relative Accuracy (WRAcc), the Weighted
Kullback-Leibler divergence (WKL), the weighted Krimp gain (WKG).

\subsection{Search strategies}
In this section, we explain beam search and the alternatives.

\section{Promising subspaces}

Our framework Turbo Subgroup operates in two steps. During the first step, it
identifies candidate subspaces.  During the second step, it scans each
candidate to reveal the subgroups. In this section, we explain how to reveal
the promising subspaces.

\subsection{Subgroup quality and mutual information}
First, we define the class of quality measures on which our work applies.
\begin{definition}
    Let $G$ describe any sugbroup based on the columns $\gamma(G) = \{C_0,
\ldots, C_k\}$.  A quality measure $\varphi$ is \textbf{consistent} if and only
if it grows with $I(C_0, \ldots, C_k; T)$.  
\end{definition}

Recall that $I(T | C_0, \ldots, C_k) = H(T) - H(T | C_0, \ldots,C_k)$. In other
words, this quantity describes \emph{the influence of the subgroup variables on
the target}. If it is high, then the choice of variables is promising:
conditionning on $C_0, \ldots,C_k$ reduces the uncertainety of the target
variable.  A subgroup based on these variables is likely to reveal an
``unusual'' distribution. If is is low, this indicates that there is little
influence. Regardless of how we condition, the the subgroups cannot be very
interesting.

\begin{lemma}
    The WKL is consistent. The WRAcc is \emph{not} consistent, but its absolute value
    is. I stronly suspect that WKG gain is consistent, but this is hard to
    prove.
\end{lemma}

These observations are fundamental: they allow us to \emph{decouple} subspace search and
sugroup search. If a subspace scores high wrt. the mutual information, then it
contains at least one interesting subgroup. This is why we can operate in two
phases. During the first phase, we enumerate $k$ promising subspaces. During the
second phase, we explore these subspaces sequentially with a regular beam
search.

\subsection{Dececting promising susbpaces}

At this point we could create a naive algorithm: for every group of variables
$C_0, \ldots,C_k$, compute $I(C_0, \ldots, C_k; T)$ and report the best $k$
candidates.
Unfortunately, this algorithm is highly unefficient. First, the search space is
exponential with the number of variables. Second, the constant cost is huge:
calculating $I(C_0, \ldots, C_k; T)$ requires a very expensive cross product of
$k$ variables.

Maximizing $I(C_0, \ldots, C_k; T)$ directly is difficult.  Instead, we can
to use bounds, or approximations.

\subsubsection{Technique 1: bounds}

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    $$
        I(C_0, \ldots, C_k; T) \geq \max_{i,j\in [0,k]} I(C_i, C_j ; T)
    $$
\end{lemma}

\begin{proof}
    We have $I(C_0, \ldots, C_k; T) = H(T) - H(T|C_0, \ldots, C_k)$ Recall that
    conditioning reduces entropy. Therefore, for any $i,j$, we have $H(T|C_0,
    \ldots, C_k) \leq H(T |C_i, C_j)$. Therefore, we have:
    $I(C_0, \ldots, C_k; T) \geq H(T) - \min_{i,j\in [0,k]} H(T | C_i, C_j)$.
    The final formula is obtained by using the definition of mutual
    information.
\end{proof}

Thanks to the chain rule, we can also obtain a higher bound:

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    \[ 
    I(C_0, \ldots, C_k; T) \leq \min
    \begin{cases}
        H(T)\\
        I(C_0, C_1) + H(C_2, C_3) + \ldots + H(C_{k-1}, C_k)
    \end{cases}
   \] 
\end{lemma}

\begin{proof}
According to the chain rule, we have: 
$$
I(C_0, C_1, \ldots, C_k;T) = I(C_0, C_1 ; T) + I(C_2,\ldots,C_k ; T | C_0, C_1)
$$
By definition of the conditional mutual information, we have $I(X;Y|Z) \leq
H(X)$ for any triplet of variables. Therefore:
\[
    I(C_2,\ldots,C_k ; T | C_0, C_1) \leq H(T)
\]and also:
\[
    \begin{split}
        I(C_2,\ldots,C_k ; T | C_0, C_1) & \leq H(C_2,\ldots,C_k)\\
        & \leq H(C_2, C_3) + \ldots + H(C_{k-1},C_k)\\
    \end{split}
\]
\end{proof}

\subsubsection{Technique 2: approximations}

Unfortunately our bounds are quite weak. One alternative is to
approximate the mutual informations. We can make a Naive hypothesis:
\[
    I(C_k ; T | C_0, \ldots, C_{k-1}) = I(C_k ; T | C_{k-1})
\]

With this assumption, we have:
\[
    I(T ; C_0, \ldots, C_{k-1}) \approx I(T, C_0) + I(T, C_1 | C_0)+ \ldots +  I(T, C_k | C_{k-1})
\]

One last problem: since every permutation is possible, which one gives us the
best approximation?  Maybe we could use maximum weight
maximum spanning tree (cf. Chow Liu trees).
