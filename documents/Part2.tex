\section{Promising subspaces}

Our framework Turbo Subgroup operates in two steps. During the first step, it
identifies candidate subspaces.  During the second step, it scans each
candidate to reveal the subgroups. In this section, we explain how to reveal
the promising subspaces.

\subsection{Subgroup quality and mutual information}
First, we define the class of quality measures on which our work applies.
\begin{definition}
    Let $G$ describe any sugbroup based on the columns $\gamma(G) = \{C_0,
\ldots, C_k\}$.  A quality measure $\varphi$ is \textbf{consistent} if and only
if it grows with $I(C_0, \ldots, C_k; T)$.  
\end{definition}

Recall that $I(T | C_0, \ldots, C_k) = H(T) - H(T | C_0, \ldots,C_k)$. In other
words, this quantity describes \emph{the influence of the subgroup variables on
the target}. If it is high, then the choice of variables is promising:
conditionning on $C_0, \ldots,C_k$ reduces the uncertainety of the target
variable.  A subgroup based on these variables is likely to reveal an
``unusual'' distribution. If is is low, this indicates that there is little
influence. Regardless of how we condition, the the subgroups cannot be very
interesting.

\begin{lemma}
    The WKL is consistent. The WRAcc is \emph{not} consistent, but its absolute value
    is. I stronly suspect that WKG gain is consistent, but this is hard to
    prove.
\end{lemma}

These observations are fundamental: they allow us to \emph{decouple} subspace search and
sugroup search. If a subspace scores high wrt. the mutual information, then it
contains at least one interesting subgroup. This is why we can operate in two
phases. During the first phase, we enumerate $k$ promising subspaces. During the
second phase, we explore these subspaces sequentially with a regular beam
search.

\subsection{Dececting promising susbpaces}

At this point we could create a naive algorithm: for every group of variables
$C_0, \ldots,C_k$, compute $I(C_0, \ldots, C_k; T)$ and report the best $k$
candidates.
Unfortunately, this algorithm is highly unefficient. First, the search space is
exponential with the number of variables. Second, the constant cost is huge:
calculating $I(C_0, \ldots, C_k; T)$ requires a very expensive cross product of
$k$ variables.

Maximizing $I(C_0, \ldots, C_k; T)$ directly is difficult.  Instead, we can
to use bounds, or approximations.

\subsubsection{Technique 1: bounds}

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    $$
        I(C_0, \ldots, C_k; T) \geq \max_{i,j\in [0,k]} I(C_i, C_j ; T)
    $$
\end{lemma}

\begin{proof}
    We have $I(C_0, \ldots, C_k; T) = H(T) - H(T|C_0, \ldots, C_k)$ Recall that
    conditioning reduces entropy. Therefore, for any $i,j$, we have $H(T|C_0,
    \ldots, C_k) \leq H(T |C_i, C_j)$. Therefore, we have:
    $I(C_0, \ldots, C_k; T) \geq H(T) - \min_{i,j\in [0,k]} H(T | C_i, C_j)$.
    The final formula is obtained by using the definition of mutual
    information.
\end{proof}

Thanks to the chain rule, we can also obtain a higher bound:

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    \[ 
    I(C_0, \ldots, C_k; T) \leq \min
    \begin{cases}
        H(T)\\
        I(C_0, C_1) + H(C_2, C_3) + \ldots + H(C_{k-1}, C_k)
    \end{cases}
   \] 
\end{lemma}

\begin{proof}
According to the chain rule, we have: 
$$
I(C_0, C_1, \ldots, C_k;T) = I(C_0, C_1 ; T) + I(C_2,\ldots,C_k ; T | C_0, C_1)
$$
By definition of the conditional mutual information, we have $I(X;Y|Z) \leq
H(X)$ for any triplet of variables. Therefore:
\[
    I(C_2,\ldots,C_k ; T | C_0, C_1) \leq H(T)
\]and also:
\[
    \begin{split}
        I(C_2,\ldots,C_k ; T | C_0, C_1) & \leq H(C_2,\ldots,C_k)\\
        & \leq H(C_2, C_3) + \ldots + H(C_{k-1},C_k)\\
    \end{split}
\]
\end{proof}

\subsubsection{Technique 2: approximations}

Unfortunately our bounds are quite weak. One alternative is to
approximate the mutual informations. We can make a Naive hypothesis:
\[
    I(C_k ; T | C_0, \ldots, C_{k-1}) = I(C_k ; T | C_{k-1})
\]

With this assumption, we have:
\[
    I(T ; C_0, \ldots, C_{k-1}) \approx I(T, C_0) + I(T, C_1 | C_0)+ \ldots +  I(T, C_k | C_{k-1})
\]

One last problem: since every permutation is possible, which one gives us the
best approximation?  Maybe we could use maximum weight
maximum spanning tree (cf. Chow Liu trees).
