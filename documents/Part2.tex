\section{Promising subspaces}

Our framework Turbo Subgroup operates in two steps. During the first step, it
identifies candidate subspaces.  During the second step, it scans each
candidate to reveal the subgroups. In this section, we explain how to reveal
the promising subspaces.

\subsection{Subgroup quality and mutual information}
First, we define the class of quality measures on which our work applies.
\begin{definition}
    Let $G$ describe any sugbroup based on the columns $\gamma(G) = \{C_0,
\ldots, C_k\}$.  A quality measure $\varphi$ is \textbf{consistent} if and only
if it grows with $I(C_0, \ldots, C_k; T)$.  
\end{definition}

Recall that $I(T | C_0, \ldots, C_k) = H(T) - H(T | C_0, \ldots,C_k)$. In other
words, this quantity describes \emph{the influence of the subgroup variables on
the target}. If it is high, then the choice of variables is promising:
conditionning on $C_0, \ldots,C_k$ reduces the uncertainety of the target
variable.  A subgroup based on these variables is likely to reveal an
``unusual'' distribution. If is is low, this indicates that there is little
influence. Regardless of how we condition, the the subgroups cannot be very
interesting.

\begin{lemma}
    The WKL is consistent. The WRAcc is \emph{not} consistent, but its absolute value
    is. I stronly suspect that WKG gain is consistent, but this is hard to
    prove.
\end{lemma}

These observations are fundamental: they allow us to \emph{decouple} subspace search and
sugroup search. If a subspace scores high wrt. the mutual information, then it
contains at least one interesting subgroup. This is why we can operate in two
phases. During the first phase, we enumerate $k$ promising subspaces. During the
second phase, we explore these subspaces sequentially with a regular beam
search.

\subsection{Dececting promising susbpaces}

At this point we could create a naive algorithm: for every group of variables
$C_0, \ldots,C_k$, compute $I(C_0, \ldots, C_k; T)$ and report the best $k$
candidates.
Unfortunately, this algorithm is highly unefficient. First, the search space is
exponential with the number of variables. Second, the constant cost is huge:
calculating $I(C_0, \ldots, C_k; T)$ requires a very expensive cross product of
$k$ variables.

Maximizing $I(C_0, \ldots, C_k; T)$ directly is difficult.  Instead, we can
to use some bounds.

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    $$
        I(C_0, \ldots, C_k; T) \geq \max_{i,j\in [0,k]} I(C_i, C_j ; T)
    $$
\end{lemma}

\begin{proof}
    We have $I(C_0, \ldots, C_k; T) = H(T) - H(T|C_0, \ldots, C_k)$ Recall that
    conditioning reduces entropy. Therefore, for any $i,j$, we have $H(T|C_0,
    \ldots, C_k) \leq H(T |C_i, C_j)$. Therefore, we have:
    $I(C_0, \ldots, C_k; T) \geq H(T) - \min_{i,j\in [0,k]} H(T | C_i, C_j)$.
    The final formula is obtained by using the definition of mutual
    information.
\end{proof}

Thanks to the chain rule, we can also obtain a higher bound:

\begin{lemma}
    For any set of variables $C_0, \ldots, C_k, T$, the following property holds:
    \[ 
    I(C_0, \ldots, C_k; T) \leq \min
    \begin{cases}
        H(T)\\
        I(C_0, C_1) + H(C_2, C_3) + \ldots + H(C_{k-1}, C_k)
    \end{cases}
   \] 
\end{lemma}

\begin{proof}
According to the chain rule, we have: 
$$
I(C_0, C_1, \ldots, C_k;T) = I(C_0, C_1 ; T) + I(C_2,\ldots,C_k ; T | C_0, C_1)
$$
By definition of the conditional mutual information, we have $I(X;Y|Z) \leq
H(X)$ for any triplet of variables. Therefore:
\[
    I(C_2,\ldots,C_k ; T | C_0, C_1) \leq H(T)
\]and also:
\[
    \begin{split}
        I(C_2,\ldots,C_k ; T | C_0, C_1) & \leq H(C_2,\ldots,C_k)\\
        & \leq H(C_2, C_3) + \ldots + H(C_{k-1},C_k)\\
    \end{split}
\]
\end{proof}
These bounds are not very interesting if we want to know the exact value of
$I(C_0, \ldots, C_k; T)$. However, it allows us to do some pruning.
To explain how this works, we introduce a new construct: \emph{the
co-influence graph}.

\begin{definition}
    The \textbf{co-influence graph} $\mathcal{G}$ is a weighted undirected
    graph. Each node represents a variable $C_i$, and each edge between two
    vertices $C_i$ and $C_j$ has a weight $I(C_i, C_j ; T)$.
\end{definition}


Intuitively, the graph represents which variables ``work well'' together. A
heavy edge means that the joint distribution of the vertex-variables has a high impact on
the target variable. From $\mathcal{G}$ we can derive the \textbf{partial
co-influence graph} $\mathcal{G}(\sigma)$. This graph is a \emph{non weighted}
undirected graph. It contains all the vertices of $\mathcal{G}$, and all the
edges $\mathcal{G}$ with a weight greater than $\sigma$.
